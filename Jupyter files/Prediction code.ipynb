{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pickle\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "reddit = praw.Reddit(client_id='8kRfdrPdpbJe9Q', client_secret='qeSfFi6kizAdurAFL790ZhQL_uM', user_agent='Anukriti Jain')\n",
    "loaded_model = pickle.load(open('website/model.sav', 'rb'))\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords_eng = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "#d = enchant.Dict(\"en_US\")\n",
    "\n",
    "def getStemmedText(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    post = text\n",
    "    #post = text.split()\n",
    "    #post = \"\"\n",
    "    #for word in new_text:\n",
    "    #   if d.check(word)==True:\n",
    "    #        post = post + \" \" + word\n",
    "    \n",
    "    tokens = tokenizer.tokenize(post)\n",
    "    new_tokens = []\n",
    "    stemmed_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in stopwords_eng:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    for token in new_tokens:\n",
    "        stemmed_tokens.append(stemmer.stem(token))\n",
    "        \n",
    "    cleaned_text = ' '.join(stemmed_tokens)\n",
    "    return cleaned_text\n",
    "    \n",
    "def detect_flair(url,loaded_model):\n",
    "\n",
    "    submission = reddit.submission(url=url)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    data['title'] = submission.title\n",
    "    data['url'] = submission.url\n",
    "    data['body'] = submission.selftext\n",
    "\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    count = 0\n",
    "    for top_level_comment in submission.comments:\n",
    "        if(count<5):\n",
    "            comment = comment + ' ' + top_level_comment.body\n",
    "        count+=1\n",
    "    data[\"comment\"] = comment\n",
    "  \n",
    "    data['title'] = getStemmedText(data['title'])\n",
    "    data['comment'] = getStemmedText(data['comment'])\n",
    "    data['body'] = getStemmedText(data['body'])\n",
    "    data['url'] = getStemmedText(data['url'])\n",
    "    \n",
    "    data['combine'] = data['title'] + data['comment'] + data['url'] + data['body']\n",
    "  \n",
    "    return loaded_model.predict([data['combine']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = detect_flair('https://www.reddit.com/r/india/comments/d0ukr4/as_a_normal_citizen_how_afraid_should_i_be_of_the/', loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AskIndia\n"
     ]
    }
   ],
   "source": [
    "print(ans.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:renv] *",
   "language": "python",
   "name": "conda-env-renv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
